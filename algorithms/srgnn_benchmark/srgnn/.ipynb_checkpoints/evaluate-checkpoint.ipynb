{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54960a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, test_model, train_data, latencies_out_file, pred_out_file):\n",
    "    test_model.eval()\n",
    "    \n",
    "    metrics = [NDCG(), MAP(), Precision(), Recall(), HitRate(), MRR(), Coverage(training_df=train_data)\n",
    "    ,\n",
    "           Popularity(training_df=train_data)]\n",
    "    \n",
    "    predictions_writer = PredictionsWriter(outputfilename=pred_out_file, evaluation_n=20)\n",
    "    latency_writer = LatencyWriter(latencies_out_file)\n",
    "    \n",
    "    prediction_sw = SimpleStopwatch()\n",
    "    \n",
    "    score_exp = []\n",
    "    correct = 0\n",
    "    for _, data in enumerate(loader):\n",
    "        data.to('cpu') #initial cuda\n",
    "        with torch.no_grad():\n",
    "            prediction_sw.start()\n",
    "            score = test_model(data)\n",
    "            pred = score.max(dim=1)[1]\n",
    "            label = data.y\n",
    "            next_items = data.y_next\n",
    "            \n",
    "            prediction_sw.stop(0)\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "        sub_scores = score.topk(20)[1]\n",
    "        sub_scores = sub_scores.cpu().detach().numpy()\n",
    "\n",
    "        for ele in range(sub_scores.shape[0]):\n",
    "            top_k_pred = sub_scores[ele]\n",
    "            recommendations = pd.Series(0.0, top_k_pred)\n",
    "            \n",
    "            \n",
    "            if isinstance(next_items[ele], int) == True:\n",
    "                n_item = [next_items[ele]]\n",
    "            else:\n",
    "                n_item = next_items[ele]\n",
    "\n",
    "            predictions_writer.appendline(recommendations, n_item)\n",
    "            \n",
    "\n",
    "            for metric in metrics:\n",
    "                metric.add(recommendations, np.array(n_item))\n",
    "                \n",
    "    scores = []\n",
    "    for metric in metrics:\n",
    "        metric_name, score = metric.result()\n",
    "        scores.append(\"%.4f\" % score)\n",
    "        print(metric_name, \"%.4f\" % score)\n",
    "        \n",
    "    predictions_writer.close()\n",
    "    score_exp.append(scores)\n",
    "        \n",
    "        \n",
    "    for (position, latency) in prediction_sw.get_prediction_latencies_in_micros():\n",
    "        latency_writer.append_line(position, latency)\n",
    "    latency_writer.close()\n",
    "        \n",
    "    with open('results_srgnn/srgnn_model_performance_over_all_slices.csv', 'a+') as f:\n",
    "        f.write(\",\".join(str(item) for item in scores))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
